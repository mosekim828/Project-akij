import os
import time
import json
import re
import subprocess
import glob
import shutil
import random
import requests
from datetime import datetime
from collections import Counter
from typing import List, Dict, TypedDict, Any, Tuple, Union
from mlx_lm import load, generate

# â˜… RAG ê²€ìƒ‰ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
try:
    from langchain_community.vectorstores import Chroma
    from langchain_community.embeddings import HuggingFaceEmbeddings
except ImportError:
    print("âš ï¸ [System] LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. 'pip install langchain-community chromadb' í•„ìš”.")

import fitz  # PyMuPDF

# ==============================================================================
# [0] í•µì‹¬ ì„¤ì • (USER CONFIGURATION)
# ==============================================================================

# 1. PC í™”ê°€ ì„œë²„ (â˜… ìˆ˜ì •ë¨: ë¡œê·¸ ê¸°ë°˜ 192.168.0.65ë¡œ ì„¤ì •)
# IPê°€ ë°”ë€Œë©´ ì—¬ê¸°ë¥¼ ê¼­ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
PC_FLUX_SERVER_URL = "http://192.168.0.65:8000/draw"

# 2. ê¸°ë³¸ ì‚½í™” ë¹ˆë„
DEFAULT_ILLUSTRATION_FREQ = 3 

# 3. ê²½ë¡œ ì„¤ì •
USER_HOME = "/Users/juson"
MODEL_PATH = f"{USER_HOME}/.cache/huggingface/hub/models--mlx-community--Llama-4-Maverick-17B-16E-Instruct-6bit/snapshots/542ea389fcd614c665c4306bd60ad053d9da8d03"

FACTORY_DIR = f"{USER_HOME}/Desktop/factory_input"
DIR_RESULT = os.path.join(FACTORY_DIR, "1_Result")
DIR_REFERENCE = os.path.join(FACTORY_DIR, "2_Reference_Style")

GENESIS_PATH = f"{USER_HOME}/Desktop/Genesis_Project"
DIR_TEMPLATE = os.path.join(GENESIS_PATH, "D0_template")
DIR_FONTS = os.path.join(GENESIS_PATH, "D5_Fonts")

# â˜… Vector DB ê²½ë¡œ & ì„ë² ë”© ëª¨ë¸
DB_PERSIST_DIR = f"{USER_HOME}/Desktop/Genesis_Project/99_VectorDB"
EMBEDDING_MODEL_ID = "BAAI/bge-m3"

# 4. í°íŠ¸/ê¸°íƒ€
DEFAULT_FONT_TITLE = "AppleSDGothicNeo-Bold"
DEFAULT_FONT_BODY = "AppleMyungjo"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# 5. ë§ì¶¤ë²• ê²€ì‚¬ê¸°
try:
    from hanspell import spell_checker
    HANSPELL_AVAILABLE = True
except ImportError:
    HANSPELL_AVAILABLE = False
    print("âš ï¸ [System] py-hanspell ë¯¸ì„¤ì¹˜. AI êµì •ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")


# ==============================================================================
# [1] ë„êµ¬ í´ë˜ìŠ¤ (TOOLKIT)
# ==============================================================================

class StyleReplicator:
    """PDF ìŠ¤íƒ€ì¼(ì—¬ë°±, í°íŠ¸ í¬ê¸°) ë¶„ì„ê¸°"""
    @staticmethod
    def analyze_pdf(pdf_path):
        try:
            doc = fitz.open(pdf_path)
            page = doc[0] # ì²« í˜ì´ì§€ë§Œ ë¶„ì„
            blocks = page.get_text("dict")["blocks"]
            font_sizes = []
            
            # í°íŠ¸ í¬ê¸° ìˆ˜ì§‘
            for b in blocks:
                if "lines" in b:
                    for l in b["lines"]:
                        for s in l["spans"]:
                            font_sizes.append(s["size"])
            if not font_sizes: return None
            
            # ê°€ì¥ ë§ì´ ì“°ì¸ í¬ê¸°(ë³¸ë¬¸)ì™€ ê°€ì¥ í° í¬ê¸°(ì œëª©) ì¶”ì¶œ
            body_size = Counter(font_sizes).most_common(1)[0][0]
            title_size = max(font_sizes)
            
            # ì—¬ë°± ê³„ì‚° (ì¢Œìš° ì—¬ë°± ì¶”ì •)
            rect = page.rect
            margin_x = 72 # ê¸°ë³¸ê°’
            if blocks:
                left_x = blocks[0]["bbox"][0]
                right_x = blocks[0]["bbox"][2]
                margin_x = (rect.width - (right_x - left_x)) / 2
            
            return {
                "filename": os.path.basename(pdf_path),
                "page_width": f"{rect.width}pt", 
                "page_height": f"{rect.height}pt",
                "body_size": f"{body_size:.1f}pt", 
                "title_size": f"{title_size:.1f}pt",
                "margin": f"{margin_x / 2.83:.1f}mm" # pt to mm ë³€í™˜
            }
        except: return None

class TermGuard:
    @staticmethod
    def enforce(text: str) -> str:
        replacements = {
            "ë…¸ê°€": "ë…¸ì•„", "ì„¸ì‹ ì": "ìƒˆì‹ ì", "ê¸°ë¥¼ ì¶•ë³µí•©ë‹ˆë‹¤": "ì£¼ë‹˜ì˜ ì´ë¦„ìœ¼ë¡œ ì¶•ë³µí•©ë‹ˆë‹¤",
            "###": "", "**": ""
        }
        for k, v in replacements.items(): text = text.replace(k, v)
        text = text.replace("<", "").replace(">", "")
        text = re.sub(r"\n{3,}", "\n\n", text)
        return text

    @staticmethod
    def run_spell_check(text: str) -> str:
        if not HANSPELL_AVAILABLE: return text
        try:
            corrected = ""
            for line in text.split('\n'):
                if not line.strip(): corrected += "\n"
                elif len(line) < 500: corrected += spell_checker.check(line).checked + "\n"
                else: corrected += line + "\n"
            return corrected.strip()
        except: return text

class TextManager:
    @staticmethod
    def split_text_clean(text, chunk_size=3000):
        chunks = []
        start = 0
        text_len = len(text)
        while start < text_len:
            end = min(start + chunk_size, text_len)
            if end < text_len:
                last_period = text.rfind('.', start, end)
                if last_period != -1 and last_period > start + (chunk_size // 2): end = last_period + 1
            chunk = text[start:end].strip()
            if chunk: chunks.append(chunk)
            start = end
        return chunks

class KnowledgeManager:
    """Vector DB ê²€ìƒ‰ ë° í°íŠ¸/í…œí”Œë¦¿ ê´€ë¦¬"""
    @staticmethod
    def scan_fonts(font_dir):
        found = []
        if os.path.exists(font_dir):
            for root, _, files in os.walk(font_dir):
                for file in files:
                    if file.lower().endswith(('.ttf', '.otf')):
                        name = os.path.splitext(file)[0]
                        name = re.sub(r"-(Bold|Regular|Light|Medium|Black|Thin|ExtraBold|SemiBold)", "", name, flags=re.IGNORECASE)
                        name = re.sub(r"([a-z])([A-Z])", r"\1 \2", name) 
                        if "Noto Sans" in name: name = "Noto Sans KR"
                        if "Noto Serif" in name: name = "Noto Serif KR"
                        name = name.strip()
                        if name not in found: found.append(name)
        return found if found else [DEFAULT_FONT_TITLE, DEFAULT_FONT_BODY]

    @staticmethod
    def load_templates():
        """ë””ìì¸ í…œí”Œë¦¿ ë¡œë“œ"""
        buf = ""
        if os.path.exists(DIR_TEMPLATE):
            for file in glob.glob(os.path.join(DIR_TEMPLATE, "*.typ")):
                try: 
                    with open(file, 'r', encoding='utf-8') as f:
                        buf += f"\n[Template Code: {os.path.basename(file)}]\n{f.read()}\n"
                except: pass
        return buf

    @staticmethod
    def search_vector_db(query: str, k: int = 5) -> str:
        """RAG ì§€ì‹ ê²€ìƒ‰"""
        if not os.path.exists(DB_PERSIST_DIR):
            log("Librarian", "âš ï¸ Vector DBê°€ ì—†ìŠµë‹ˆë‹¤.")
            return ""

        log("Librarian", f"ğŸ” ì§€ì‹ ê²€ìƒ‰: '{query[:30]}...'")
        try:
            embeddings = HuggingFaceEmbeddings(
                model_name=EMBEDDING_MODEL_ID,
                model_kwargs={'device': 'mps'},
                encode_kwargs={'normalize_embeddings': True}
            )
            vectordb = Chroma(
                persist_directory=DB_PERSIST_DIR, 
                embedding_function=embeddings,
                collection_name="genesis_knowledge"
            )
            docs = vectordb.similarity_search(query, k=k)
            context_text = ""
            for i, doc in enumerate(docs):
                src = doc.metadata.get("source", "Unknown")
                cat = doc.metadata.get("category", "Unknown")
                context_text += f"\n[Reference {i+1} | {cat}/{src}]\n{doc.page_content}\n"
            
            log("Librarian", f"âœ… ê´€ë ¨ ë¬¸ì„œ {len(docs)}ê±´ í™•ë³´.")
            return context_text
        except Exception as e:
            log("Librarian", f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return ""

    @staticmethod
    def fix_typst_syntax(code: str) -> str:
        """[V16] ë¬¸ë²• ì˜¤ë¥˜ ë° ì°Œêº¼ê¸° ì™„ë²½ ì œê±°"""
        cleaned_lines = []
        for line in code.split('\n'):
            stripped = line.strip()
            # ë§ˆí¬ë‹¤ìš´ ì œê±°
            if stripped.startswith("###") or stripped.startswith("##") or stripped.startswith("```"):
                continue
            cleaned_lines.append(line)
        
        code = "\n".join(cleaned_lines)

        # 1. êµ¬ë²„ì „ ë¬¸ë²•(locate) êµì²´
        code = code.replace("locate(loc =>", "#context")
        code = re.sub(r"#?locate\s*\(\s*\w+\s*=>", "#context", code)
        
        # 2. â˜… ê´„í˜¸ ì¤‘ì²© ì—ëŸ¬ í•´ê²° (loc ë³€ìˆ˜ ê°•ì œ ì‚­ì œ)
        code = code.replace(", loc)", ")") 
        code = code.replace(",loc)", ")")
        
        # 3. ë‚˜ë¨¸ì§€ ì •ë¦¬
        code = code.replace(".at(loc)", ".get()")
        code = code.replace("locate(heading)", "heading")
        code = code.replace("indent: true,", "")
        code = code.replace("indent: true", "")
        
        return code
    
def log(agent, msg):
    icons = {"Director": "ğŸ¬", "Librarian": "ğŸ“š", "Editor": "âœï¸", "Designer": "ğŸ¨", "System": "âš™ï¸", "Replicator": "ğŸ§¬", "Illustrator": "ğŸ–Œï¸"}
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {icons.get(agent, 'ğŸ¤–')} [{agent}] {msg}")

def cleanup_on_start():
    for d in [FACTORY_DIR, DIR_RESULT, DIR_REFERENCE, DIR_TEMPLATE]:
        if not os.path.exists(d): os.makedirs(d)
    target = os.path.join(FACTORY_DIR, "pc_output.json")
    if os.path.exists(target):
        try:
            timestamp = int(time.time())
            os.rename(target, os.path.join(FACTORY_DIR, f"ignored_{timestamp}.json"))
        except: pass

# ==============================================================================
# [2] ìƒíƒœ ê´€ë¦¬
# ==============================================================================
class BookState(TypedDict):
    user_instruction: str; raw_material: str;
    knowledge_context: str; available_fonts: List[str]
    book_title: str; text_chunks: List[str]
    polished_chunks: List[Tuple[str, str]]
    current_chunk_idx: int;
    replicated_template_name: str; illustration_freq: int

# ==============================================================================
# [3] AI ì—ì´ì „íŠ¸
# ==============================================================================

def load_model_once():
    try:
        log("System", "Maverick(Llama-4) ì—”ì§„ ì˜ˆì—´ ì¤‘...")
        return load(MODEL_PATH)
    except: return None, None

def agent_librarian(model, tokenizer, state: BookState) -> BookState:
    log("Librarian", "ì§€ì‹ ë„ì„œê´€ ì ‘ì† ì¤‘...")
    state['available_fonts'] = KnowledgeManager.scan_fonts(DIR_FONTS)
    tpl_context = KnowledgeManager.load_templates()
    search_query = f"{state['user_instruction']} {state['raw_material'][:500]}"
    retrieved_knowledge = KnowledgeManager.search_vector_db(search_query, k=6)
    state['knowledge_context'] = tpl_context + "\n" + retrieved_knowledge
    return state

def agent_replicator(model, tokenizer, state: BookState) -> BookState:
    state['replicated_template_name'] = ""
    
    # PDF ì°¾ê¸°
    pdf_files = glob.glob(os.path.join(DIR_REFERENCE, "*.pdf"))
    if not pdf_files:
        log("Replicator", "âš ï¸ ìŠ¤íƒ€ì¼ ì°¸ê³ ìš© PDFê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©.")
        return state

    target_pdf = random.choice(pdf_files)
    log("Replicator", f"ìŠ¤íƒ€ì¼ ì¶”ì¶œ ëŒ€ìƒ: {os.path.basename(target_pdf)}")
    
    analysis = StyleReplicator.analyze_pdf(target_pdf)
    
    if analysis:
        fonts_str = ", ".join(state['available_fonts'][:5]) 
        prompt = f"""<|system|>Typst Expert.
[Data]: {analysis}
[Fonts]: {fonts_str}
[Task]: Write Typst code for Page Setup (#set page, #set text, #set par) and Title alignment.
[STRICT RULES]:
1. Output ONLY valid Typst code.
2. NO Markdown headers.
3. Use 'context' instead of 'locate'.
4. DO NOT use 'indent: true'.
<|user|>Write Code<|assistant|>"""
        
        try:
            typ_code = generate(model, tokenizer, prompt=prompt, max_tokens=1000, verbose=False)
            typ_code = KnowledgeManager.fix_typst_syntax(typ_code) 
            template_name = f"replicated_{int(time.time())}.typ"
            with open(os.path.join(DIR_TEMPLATE, template_name), "w", encoding="utf-8") as f: 
                f.write(typ_code)
            state['replicated_template_name'] = template_name
            log("Replicator", "âœ… ìŠ¤íƒ€ì¼ í…œí”Œë¦¿ ìƒì„± ì™„ë£Œ.")
        except Exception as e:
            log("Replicator", f"âš ï¸ í…œí”Œë¦¿ ìƒì„± ì‹¤íŒ¨: {e}")

    return state

def agent_director(model, tokenizer, state: BookState) -> BookState:
    if not state['raw_material']: return state
    log("Director", "ê¸°íš ë° ì „ì²˜ë¦¬ ì¤‘... (ì‘ì—…ì§€ì‹œì„œ ë¶„ì„ ì¤‘)")
    
    # â˜… í”„ë¡¬í”„íŠ¸ ê°•í™”: ì§€ì‹œì„œ ì œëª© ì ˆëŒ€ ì¤€ìˆ˜
    prompt = f"""<|system|>You are a Chief Editor. Determine the book title.
[User Instruction]: "{state['user_instruction']}"
[Content Preview]: {state['raw_material'][:2000]}

[STRICT RULES]
1. IF the User Instruction specifies a title (e.g. "ì œëª©: ..."), USE IT EXACTLY.
2. IF NO title is specified, create a best-selling title.
3. Output ONLY the final title text. NO tags, NO explanation.
<|user|>Determine Title<|assistant|>"""
    
    title_raw = generate(model, tokenizer, prompt=prompt, max_tokens=100, verbose=False).strip()
    
    # â˜… íƒœê·¸ ë° ì°Œêº¼ê¸° ì œê±°
    title_clean = title_raw.split("<|")[0]
    title_clean = re.sub(r"^(Title|Subject|ì œëª©|ì±…\s*ì œëª©|ê²°ê³¼)\s*[:ï¼š]\s*", "", title_clean, flags=re.IGNORECASE)
    title_clean = title_clean.strip('"\' ')
    if "\n" in title_clean: title_clean = title_clean.split("\n")[0].strip()

    state['book_title'] = title_clean
    log("Director", f"ì±… ì œëª© í™•ì •: {state['book_title']}")
    
    state['text_chunks'] = TextManager.split_text_clean(state['raw_material'])
    return state

def agent_hybrid_editor(model, tokenizer, state: BookState) -> BookState:
    idx = state['current_chunk_idx']
    if idx >= len(state['text_chunks']): return state
    
    current_text = state['text_chunks'][idx]
    log("Editor", f"ìœ¤ë¬¸ ì‘ì—… ì¤‘... [{idx+1}/{len(state['text_chunks'])}]")
    
    prev_text = ""
    if state['polished_chunks']:
        for t, c in reversed(state['polished_chunks']):
            if t == 'text':
                prev_text = c[-300:]
                break
    
    prompt = f"""<|system|>ìˆ˜ì„ í¸ì§‘ì¥ì…ë‹ˆë‹¤. ë¬¸ë§¥ ë³µì› ë° ìœ¤ë¬¸.
[ì°¸ê³  ì§€ì‹]: {state['knowledge_context'][:2000]}
[ì›ì¹™] ë¬¸ë§¥ ë³µì›, ë¬¸ì–´ì²´, ë¬¸ë‹¨ êµ¬ë¶„.
<|user|>[ì´ì „]:...{prev_text}\n[ì›ë¬¸]:{current_text}\n[ì§€ì‹œ]:ìœ¤ë¬¸í•˜ë¼.<|assistant|>"""
    
    draft = generate(model, tokenizer, prompt=prompt, max_tokens=4000, verbose=False)
    draft = re.sub(r"^(ë„¤|ë¬¼ë¡ |ì•Œê² |í™•ì¸|ìˆ˜ì •|ìœ¤ë¬¸|ì œì‹œ|ë”°ë¼ì„œ).+?(\n|$)", "", draft, flags=re.MULTILINE).strip()
    
    if HANSPELL_AVAILABLE: 
        draft = TermGuard.run_spell_check(draft)
    
    final_text = TermGuard.enforce(draft)
    state['polished_chunks'].append(('text', final_text))
    state['current_chunk_idx'] += 1
    return state

def agent_illustrator(model, tokenizer, state: BookState) -> BookState:
    """â˜… íƒ€ì„ì•„ì›ƒ 180ì´ˆ & í•œê¸€ ë°©ì–´ ì ìš©"""
    processed_count = len(state['polished_chunks'])
    if not state['polished_chunks']: return state

    last_type, last_content = state['polished_chunks'][-1]
    freq = state.get('illustration_freq', DEFAULT_ILLUSTRATION_FREQ)

    if last_type == 'text' and (processed_count % freq == 0):
        log("Illustrator", f"ğŸ¨ ë¬¸ë§¥ ë¶„ì„ ë° ì‚½í™” ì˜ë¢° (ì„¤ì • ë¹ˆë„: {freq})...")
        
        prompt_desc = f"""<|system|>Visual Director.
Create a text-to-image prompt based on context.
[Context]: {last_content[:500]}
[STRICT RULES]: Output English prompt only. No Korean. Biblical oil painting style.
<|user|>Write prompt<|assistant|>"""
        
        try:
            visual_prompt = generate(model, tokenizer, prompt=prompt_desc, max_tokens=150, verbose=False).strip()
        except: visual_prompt = "Error"

        # í•œê¸€/ì˜¤ë¥˜ ë°©ì–´
        if re.search(r'[ê°€-í£]', visual_prompt) or len(visual_prompt) < 5:
            visual_prompt = "A holy biblical scene, oil painting style, cinematic lighting, solemn atmosphere"
        
        visual_prompt = re.sub(r'^(Here is|Prompt:).*?[\:\n]', '', visual_prompt).strip()
        log("Illustrator", f"ì˜ë¢°ì„œ ì „ì†¡: {visual_prompt[:40]}...")

        try:
            # â˜… íƒ€ì„ì•„ì›ƒ 180ì´ˆë¡œ ì—°ì¥
            res = requests.post(PC_FLUX_SERVER_URL, json={"prompt": visual_prompt}, timeout=180)
            if res.status_code == 200:
                fname = res.json().get("filename")
                if fname:
                    log("Illustrator", f"âœ… ê·¸ë¦¼ ë„ì°©: {fname}")
                    state['polished_chunks'].append(('image', fname))
            else: 
                log("Illustrator", f"âš ï¸ ì„œë²„ ì˜¤ë¥˜: {res.status_code}")
        except requests.exceptions.Timeout:
            log("Illustrator", "â° ì‹œê°„ ì´ˆê³¼ (PCê°€ ë°”ì¨, ê·¸ë¦¼ ìƒëµ)")
        except Exception as e: 
            log("Illustrator", f"âŒ ì—°ê²° ì‹¤íŒ¨: {e}")
            
    return state

def agent_designer(model, tokenizer, state: BookState) -> bool:
    log("Designer", "ğŸ“š ìµœì¢… ì¡°íŒ ì‘ì—… ì‹œì‘.")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_file = os.path.join(DIR_RESULT, f"GenesisBook_{timestamp}")
    
    # í°íŠ¸ ì„¤ì •
    state['available_fonts'] = KnowledgeManager.scan_fonts(DIR_FONTS)
    fonts = state['available_fonts']
    f_title = fonts[0] if fonts else DEFAULT_FONT_TITLE
    f_body = fonts[1] if len(fonts) > 1 else DEFAULT_FONT_BODY

    style_code = ""
    if state.get('replicated_template_name'):
        try:
            tpl_path = os.path.join(DIR_TEMPLATE, state['replicated_template_name'])
            with open(tpl_path, 'r', encoding='utf-8') as f:
                style_code = f.read()
        except: pass

    if not style_code or len(style_code) < 10:
        style_code = f"""
        #set page(paper: "a4", margin: 2.5cm)
        #set text(font: "{f_body}", size: 10.5pt, lang: "ko")
        #set par(justify: true, first-line-indent: 1em, leading: 0.8em)
        """

    content_code = ""
    for c_type, content in state['polished_chunks']:
        if c_type == 'text':
            clean = content.replace("#", "").replace("*", "").replace("`", "") # ë§ˆí¬ë‹¤ìš´ ì œê±°
            clean = clean.replace("[", "\\[").replace("]", "\\]")
            content_code += f"{clean}\n\n"
        elif c_type == 'image':
            if os.path.exists(os.path.join(FACTORY_DIR, content)):
                content_code += f"#v(1em)\n#figure(image(\"../{content}\", width: 90%))\n#v(1em)\n\n"

    full_typst = f"""
    // Generated Style
    {style_code}
    
    // Title Page
    #align(center + horizon)[
      #text(font: "{f_title}", size: 24pt, weight: "bold")[{state['book_title']}]
      #v(2em)
      #text(size: 12pt)[GENESIS AI WRITER]
    ]
    #pagebreak()
    
    // Content
    #set page(numbering: "1")
    #counter(page).update(1)
    
    #align(center)[#text(font: "{f_title}", size: 18pt)[{state['book_title']}]]
    #v(1em)
    
    {content_code}
    """
    
    with open(f"{out_file}.typ", "w", encoding="utf-8") as f: f.write(full_typst)
    try:
        subprocess.run(["typst", "compile", f"{out_file}.typ", f"{out_file}.pdf", "--root", FACTORY_DIR, "--font-path", DIR_FONTS], check=True)
        log("Designer", f"ğŸ‰ ì±… ì™„ì„±: {os.path.basename(out_file)}.pdf")
        return True
    except Exception as e:
        log("Designer", f"âŒ ì»´íŒŒì¼ ì‹¤íŒ¨: {e}")
        return False

# ==============================================================================
# [5] ë©”ì¸ ë£¨í”„ (V16: INFINITE LOOP FIX, TEXT MISSING FIX, IP CHECK)
# ==============================================================================
# ==============================================================================
# [5] ë©”ì¸ ë£¨í”„ (V17: ìœ ë ¹ ì§€ì‹œì„œ ë¬´ì‹œ & ëŒ€ê¸° ëª¨ë“œ ê°•í™”)
# ==============================================================================
def run_genesis_architect(model, tokenizer, input_json):
    # 1. í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”© ë¡œì§
    raw_text = input_json.get("script_ko", "") or input_json.get("script", "")
    text_filename = input_json.get("text_file", "")
    
    # í…ìŠ¤íŠ¸ íŒŒì¼ì´ ëª…ì‹œëœ ê²½ìš° ì½ê¸° ì‹œë„
    if not raw_text and text_filename:
        txt_path = os.path.join(FACTORY_DIR, text_filename)
        # íŒŒì¼ì´ ì „ì†¡ë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸° (ìµœëŒ€ 5ì´ˆ)
        for _ in range(5):
            if os.path.exists(txt_path): break
            time.sleep(1)
            
        if os.path.exists(txt_path):
            log("System", f"ğŸ“‚ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ì„±ê³µ: {text_filename}")
            with open(txt_path, 'r', encoding='utf-8') as f:
                raw_text = f.read()
        else:
            log("System", f"âš ï¸ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {text_filename}")
            return False 

    # 2. â˜… ì—¬ê¸°ê°€ í•µì‹¬: í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ì¡°ìš©íˆ ì¢…ë£Œ (False ë°˜í™˜)
    if not raw_text:
        return False

    # 3. ì‘ì—… ì‹œì‘
    custom_freq = DEFAULT_ILLUSTRATION_FREQ
    if "frequency" in input_json:
        try: custom_freq = int(input_json["frequency"])
        except: pass

    state: BookState = {
        "user_instruction": input_json.get("script", ""), 
        "raw_material": raw_text,  
        "img_snap": input_json.get("image_source", ""),
        "img_flux": input_json.get("flux_source", ""),
        "learned_style": "", "knowledge_context": "", "available_fonts": [],
        "book_title": "", "text_chunks": [], "polished_chunks": [],
        "current_chunk_idx": 0, "layout_config": {}, 
        "replicated_template_name": "", "selected_style_name": "",
        "illustration_freq": custom_freq
    }
    
    state = agent_librarian(model, tokenizer, state) 
    state = agent_replicator(model, tokenizer, state)
    state = agent_director(model, tokenizer, state)
    
    total = len(state['text_chunks'])
    while state['current_chunk_idx'] < total:
        state = agent_hybrid_editor(model, tokenizer, state)
        state = agent_illustrator(model, tokenizer, state)
    
    return agent_designer(model, tokenizer, state)

def main():
    print("\n" + "="*80)
    print(" ğŸ›ï¸  [GENESIS WRITER V17: IDLE FIX]")
    print(f"     Monitoring: {FACTORY_DIR}")
    print("     Status: ğŸŸ¢ Standby (Waiting for TEXT)")
    print("="*80)
    
    model, tokenizer = load_model_once()
    if not model: return
    cleanup_on_start()
    
    while True:
        try:
            files = os.listdir(FACTORY_DIR)
            
            # ì˜¤ì§ pc_output.jsonë§Œ ì²˜ë¦¬
            if "pc_output.json" in files:
                target_path = os.path.join(FACTORY_DIR, "pc_output.json")
                processing_path = os.path.join(FACTORY_DIR, "processing.json")
                
                try:
                    # 1. ì¼ë‹¨ ë‚šì•„ì±” (ì¤‘ë³µ ë°©ì§€)
                    os.rename(target_path, processing_path)
                    
                    # 2. ë‚´ìš©ë¬¼ ê²€ì‚¬ (ì±… ë§Œë“¤ ê±°ë¦¬ê°€ ìˆëŠ”ì§€?)
                    with open(processing_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)

                    # â˜… [í•µì‹¬ í•„í„°] í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ "ì±… ë§Œë“œëŠ” ì§€ì‹œ"ê°€ ì•„ë‹˜ -> ë¬´ì‹œ
                    has_text = data.get("script_ko") or data.get("script") or data.get("text_file")
                    
                    if not has_text:
                        # ì¡°ìš©íˆ ë¬´ì‹œí•˜ê³  íŒŒì¼ëª… ë³€ê²½ (ë¡œê·¸ë„ ì•ˆ ë‚¨ê¹€)
                        ignored_name = f"ignored_signal_{int(time.time())}.json"
                        os.rename(processing_path, os.path.join(FACTORY_DIR, ignored_name))
                        # ëŒ€ê¸° ëª¨ë“œ ìœ ì§€
                        continue

                    # 3. ì§„ì§œ ì±… ë§Œë“¤ê¸° ì§€ì‹œë¼ë©´ ì‹œì‘
                    log("System", "ğŸš€ ì›ê³  ë°œê²¬! ì§‘í•„ ì‹œì‘...")
                    success = run_genesis_architect(model, tokenizer, processing_path)
                    
                    if success:
                        final_name = f"done_{int(time.time())}.json"
                        os.rename(processing_path, os.path.join(FACTORY_DIR, final_name))
                        log("System", "âœ… ì‘ì—… ì™„ë£Œ. ëŒ€ê¸° ëª¨ë“œ ì „í™˜.")
                    else:
                        error_name = f"error_{int(time.time())}.json"
                        os.rename(processing_path, os.path.join(FACTORY_DIR, error_name))
                        log("System", "âŒ ì‘ì—… ì‹¤íŒ¨ (ì—ëŸ¬ íŒŒì¼ ë³´ê´€).")
                        
                except OSError: pass
                except Exception as e:
                    log("System", f"âš ï¸ ì˜¤ë¥˜: {e}")

        except Exception: pass
        time.sleep(1)

if __name__ == "__main__":
    main()
