# íŒŒì¼ëª…: flux_server.py (HP Omen)

import os
import torch
from fastapi import FastAPI
from pydantic import BaseModel
from diffusers import FluxPipeline

# ==========================================
# [ì„¤ì •] ëŒ€í‘œë‹˜ì´ ë§ì”€í•˜ì‹  ê·¸ ëª¨ë¸ ID (Hugging Face)
# ==========================================
# ë¡œê·¸ì¸ì´ ë˜ì–´ ìˆì–´ì•¼ë§Œ ì ‘ê·¼ ê°€ëŠ¥í•œ ëª¨ë¸ì…ë‹ˆë‹¤.
MODEL_ID = "black-forest-labs/FLUX.2-dev"

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ASSET_DIR = os.path.join(BASE_DIR, "2_assets")
os.makedirs(ASSET_DIR, exist_ok=True)

# ==========================================
# 2. ëª¨ë¸ ë¡œë“œ (ì¸ì¦ ì •ë³´ ì‚¬ìš©)
# ==========================================
print(f"ğŸš€ [System] Hugging Face ì„œë²„ì—ì„œ '{MODEL_ID}' ì ‘ì† ì‹œë„ ì¤‘...")
print("â„¹ï¸ (ì°¸ê³ : í„°ë¯¸ë„ì— 'huggingface-cli login'ì´ ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤)")

try:
    # use_auth_token=True ì˜µì…˜ì„ ì¼œì„œ ë¹„ê³µê°œ/Gated ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    pipe = FluxPipeline.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.bfloat16,
        use_auth_token=True 
    )
    
    # RTX 5090 ìµœì í™”
    pipe.enable_model_cpu_offload()
    
    print(f"âœ… [System] '{MODEL_ID}' ë¡œë“œ ì„±ê³µ! (Ready)")

except OSError as e:
    print(f"\nâŒ [Error] ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")
    print(f"ë§í¬: https://huggingface.co/{MODEL_ID}")
    print("í•´ê²°ë²•: í„°ë¯¸ë„ì— 'huggingface-cli login'ì„ ì…ë ¥í•˜ì—¬ ì¸ì¦í•´ì£¼ì„¸ìš”.")
    print(f"ì—ëŸ¬ ìƒì„¸: {e}")
    exit()
except Exception as e:
    print(f"\nâŒ [Error] ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
    exit()

# ==========================================
# 3. ì„œë²„ ê°€ë™
# ==========================================
app = FastAPI()

class ImageRequest(BaseModel):
    prompt: str
    filename: str

@app.post("/generate")
def generate_image(request: ImageRequest):
    print(f"\nğŸ“© [Request] ìš”ì²­: {request.filename}")
    save_path = os.path.join(ASSET_DIR, request.filename)

    if os.path.exists(save_path):
        return {"status": "exists", "path": save_path}

    print(f"ğŸ¨ [Flux.2] ê·¸ë¦¬ëŠ” ì¤‘... '{request.prompt[:30]}...'")
    
    image = pipe(
        request.prompt,
        height=1024,
        width=1024,
        guidance_scale=3.5,
        num_inference_steps=25,
        generator=torch.Generator("cpu").manual_seed(42)
    ).images[0]
    
    image.save(save_path)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_path}")

    return {"status": "success", "path": save_path}

# ì‹¤í–‰: uvicorn flux_server:app --host 0.0.0.0 --port 8000
