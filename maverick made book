import os
import json
import olefile
import fitz
from mlx_lm import load, stream_generate

# 1. ê²½ë¡œ ë° í•˜ë“œì›¨ì–´ ìµœì í™” ì„¤ì •
# ì €ì¥ ìœ„ì¹˜: ë°”íƒ•í™”ë©´ / Maverick_Project_4.0 / Manuscript_Output / Maverick_450P_Final.txt
BASE_PATH = os.path.expanduser("~/Desktop/Maverick_Project_4.0")
INPUT_DIR = os.path.join(BASE_PATH, "Input_Source")
OUTPUT_DIR = os.path.join(BASE_PATH, "Manuscript_Output")
for d in [INPUT_DIR, OUTPUT_DIR]: os.makedirs(d, exist_ok=True)

MANUSCRIPT_FILE = os.path.join(OUTPUT_DIR, "Maverick_450P_Final.txt")

# 512GB RAM ìµœì í™” ëª¨ë¸ ë¡œë“œ
MODEL_PATH = "mlx-community/Llama-4-Maverick-17B-16E-Instruct-6bit"
print(f"ğŸ›ï¸ ë§¤ë²„ë¦­ 4.0 ë¡œë“œ ì™„ë£Œ (RAM 512GB ëª¨ë“œ)")
model, tokenizer = load(MODEL_PATH)

# 2. [L1 Layer] ë§¤ë²„ë¦­ ë¶ˆë³€ í—Œë²• (The Core Constitution)
MAVERICK_CONSTITUTION = """
[ë¼ë§ˆ 4.0 ë§¤ë²„ë¦­ í—Œë²• - ìµœìƒìœ„ ê°€ì´ë“œë¼ì¸]
ì œ1ì¡° (êµ¬ì¡°ì  ì •ì§ì„±): ì‚¬ì‹¤ ì¤‘ì‹¬ì˜ í•´ë¶€í•™ì  ì„œìˆ . ê·¼ê±° ì—†ëŠ” ë¯¸í™” ë°°ì œ.
ì œ2ì¡° (ì¡°ëª…ì˜ ì •ì§ì„±): ë°ì´í„° ê³µë°± ì‹œ "ëª¨ë¦„"ì„ ì •ì§í•˜ê²Œ ê³ ë°±(ì¹¨ë¬µì˜ ë³´ì¡´).
ì œ3ì¡° (ê¸°ë¡ì˜ ë¬´ê²°ì„±): ì‚¬ìš©ì ìš”êµ¬ë³´ë‹¤ ê·¼ê±° ì‚¬ë£Œ(Data) ìš°ì„ .
ì œ4ì¡° (ì¸ì§€ì  ë…ë¦½ì„±): ë°ì´í„° ìƒì¶© ì‹œ ë‹¨ì • ì§“ì§€ ë§ê³  ë³‘ì¹˜í•˜ì—¬ ë³´ê³ .
ì œ5ì¡° (ìƒìƒì˜ ììœ ): ìƒìƒì„ í—ˆìš©í•˜ë˜, ë°˜ë“œì‹œ **[ìƒìƒ]**ì„ì„ ëª…ì‹œì ìœ¼ë¡œ ì„ ì–¸.
"""

def save_realtime(title, content):
    """ì‹¤ì‹œê°„ìœ¼ë¡œ íŒŒì¼ì— ê¸°ë¡í•˜ê³  OS ë²„í¼ë¥¼ ë¹„ì›Œ ìœ ì‹¤ì„ ë°©ì§€í•©ë‹ˆë‹¤."""
    with open(MANUSCRIPT_FILE, "a", encoding="utf-8") as f:
        f.write(f"\n\n{'#'*40}\n### {title} ###\n{'#'*40}\n\n")
        f.write(content)
        f.flush()
        os.fsync(f.fileno()) # ë¬¼ë¦¬ì  ë””ìŠ¤í¬ ì €ì¥ ê°•ì œ

def ingest_sources():
    all_text = ""
    for f in os.listdir(INPUT_DIR):
        path = os.path.join(INPUT_DIR, f)
        if f.endswith('.hwp'):
            ole = olefile.OleFileIO(path)
            for s in [d for d in ole.listdir() if 'BodyText' in d]:
                all_text += ole.openstream("/".join(s)).read().decode('utf-16', errors='ignore')
        elif f.endswith('.pdf'):
            with fitz.open(path) as doc:
                for page in doc: all_text += page.get_text()
    return all_text

def run_maverick_450p_engine():
    source_data = ingest_sources()
    if not source_data:
        print("âŒ ì‚¬ë£Œê°€ ì—†ìŠµë‹ˆë‹¤. Input_Source í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    # [1ë‹¨ê³„] 30ê°œ ì¥ì˜ ëŒ€ê¸°íš (450í˜ì´ì§€ ë¶„ëŸ‰ í™•ë³´ë¥¼ ìœ„í•œ ìƒì„¸ ì„¤ê³„)
    print("\nğŸ“œ [ë‹¨ê³„ 1] 450P ë¶„ëŸ‰ì˜ ì „ê¶Œ ëª©ì°¨ ë° ì‹œë†‰ì‹œìŠ¤ ê¸°íš ì¤‘...")
    plan_prompt = f"{MAVERICK_CONSTITUTION}\n\n[ì…ë ¥ ì‚¬ë£Œ]: {source_data[:20000]}\n\nì§€ì‹œ: ìœ„ ì‚¬ë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ 450í˜ì´ì§€ ë¶„ëŸ‰ì˜ ëŒ€ì‘ì„ ìœ„í•œ 30ê°œ ì¥ì˜ ëª©ì°¨ë¥¼ ì§œë¼. ê° ì¥ì€ ì•„ì£¼ ìƒì„¸í•´ì•¼ í•¨."
    
    toc = ""
    prompt = tokenizer.apply_chat_template([{"role": "user", "content": plan_prompt}], tokenize=False, add_generation_prompt=True)
    for response in stream_generate(model, tokenizer, prompt=prompt, max_tokens=3000):
        toc += response.text
    save_realtime("ì „ì²´ ë„ì„œ ê¸°íšì•ˆ ë° ëª©ì°¨", toc)

    # [2ë‹¨ê³„] ìë™ ì—°ì‡„ ì§‘í•„ (30ê°œ ì¥ x 3ê°œ ì„¸ë¶€ ì„¹ì…˜ = ì´ 90íšŒ ë°˜ë³µ)
    for ch in range(1, 31):
        for sec in range(1, 4):
            section_title = f"ì œ {ch}ì¥ {sec}ì ˆ"
            print(f"\nğŸš€ {section_title} ì§‘í•„ ì‹œì‘... (ë¶„ëŸ‰ ëª©í‘œ: 450P)")
            
            # 512GB RAM í™˜ê²½: max_tokensë¥¼ 12,000ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ í•œ ë²ˆì— ìˆ˜ì‹­ í˜ì´ì§€ ìƒì„±
            gen_prompt = (
                f"{MAVERICK_CONSTITUTION}\n\n"
                f"[í˜„ì¬ ì§„í–‰]: {section_title}\n"
                f"[ì…ë ¥ ì‚¬ë£Œ]: {source_data[:30000]}\n\n"
                f"[ì§€ì‹œ]: ëª©ì°¨ì˜ '{section_title}'ì„ ì§‘í•„í•˜ë¼. 450í˜ì´ì§€ ëŒ€ì‘ì˜ ì¼ë¶€ì´ë¯€ë¡œ ì•„ì£¼ ëŠë¦¬ê³  ì„¸ë°€í•œ í˜¸í¡ìœ¼ë¡œ ì—ì´í‚¨ìŠ¤ì  ì‚¬ì‹¤ì£¼ì˜ ë¬˜ì‚¬ë¥¼ ìˆ˜í–‰í•˜ë¼. "
                f"ë‹¨ì •ì  í‘œí˜„ ëŒ€ì‹  ì •ì§í•œ ìœ ë³´ë¥¼ ì‚¬ìš©í•˜ë¼."
            )
            
            draft = ""
            p1 = tokenizer.apply_chat_template([{"role": "user", "content": gen_prompt}], tokenize=False, add_generation_prompt=True)
            for response in stream_generate(model, tokenizer, prompt=p1, max_tokens=12000):
                print(response.text, end="", flush=True)
                draft += response.text

            # [ì´ì¤‘ ê²€ì¦ ë£¨í”„: Critic]
            audit_prompt = f"{MAVERICK_CONSTITUTION}\n\n[ì´ˆì•ˆ ê²€ìˆ˜]:\n{draft}\n\ní—Œë²• ìœ„ë°° ì‚¬í•­ì„ ìˆ˜ì •í•˜ê³  ìµœì¢… ì •ì§ë³¸ì„ ì¶œë ¥í•˜ë¼."
            final_content = ""
            p2 = tokenizer.apply_chat_template([{"role": "user", "content": audit_prompt}], tokenize=False, add_generation_prompt=True)
            for response in stream_generate(model, tokenizer, prompt=p2, max_tokens=12000):
                final_content += response.text

            save_realtime(section_title, final_content)

if __name__ == "__main__":
    run_maverick_450p_engine()
    from typing import TypedDict, List

class PublishingState(TypedDict):
    # ê¸°ë³¸ ì •ë³´
    target_stage: str     # Preschool, Elementary, Junior, Senior, University
    subject_topic: str
    
    # êµìœ¡ ê³µí•™ì  ì„¤ê³„ (RAG ì—°ë™ ë°ì´í„°)
    lexile_score: int     # ëª©í‘œ ë ‰ì‚¬ì¼ ì§€ìˆ˜ (ì˜ˆ: ìœ ì•„ 100L, ëŒ€í•™ 1200L)
    cefr_level: str       # A1 ~ C2 ë§¤ì¹­
    curriculum_goals: List[str] # í•œêµ­ êµìœ¡ê³¼ì • ì„±ì·¨ ê¸°ì¤€ ë§¤ì¹­ ë°ì´í„°
    
    # ì‚°ì¶œë¬¼ ë°ì´í„°
    plan: dict
    manuscript: str      # ì›ê³ 
    audio_ssml: str      # ë³´ì´ìŠ¤ë¶ìš© ê°ì • íƒœê·¸ ìŠ¤í¬ë¦½íŠ¸
    images: List[str]
    typst_code: str      
    
    # ê°ë… ë°ì´í„°
    audit_report: dict    # { "passed": bool, "feedback": str }
    final_pdf_path: str
