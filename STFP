import os
import time
import json
import re
import subprocess
import glob
import shutil
import random
import requests
from datetime import datetime
from collections import Counter
from typing import List, Dict, TypedDict, Any, Tuple, Union
from mlx_lm import load, generate

# ==============================================================================
# [ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ] RAG (ê²€ìƒ‰ ì¦ê°• ìƒì„±) ê´€ë ¨ ëª¨ë“ˆ
# ==============================================================================
try:
    from langchain_community.vectorstores import Chroma
    from langchain_community.embeddings import HuggingFaceEmbeddings
except ImportError:
    print("âš ï¸ [System] LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. 'pip install langchain-community chromadb' í•„ìš”.")

import fitz  # PyMuPDF (PDF ì²˜ë¦¬ìš©)

# ==============================================================================
# [0] í•µì‹¬ ì„¤ì • (USER CONFIGURATION)
# ==============================================================================

# 1. PC í™”ê°€ ì„œë²„ ì„¤ì • (IP ì£¼ì†Œ í™•ì¸ í•„ìˆ˜)
# Omen PCì˜ IPê°€ 65ë²ˆì´ ë§ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
PC_FLUX_SERVER_URL = "http://192.168.0.65:8000/draw"

# 2. ê¸°ë³¸ ì‚½í™” ë¹ˆë„ (ëª‡ ë¬¸ë‹¨ë§ˆë‹¤ ê·¸ë¦¼ì„ ê·¸ë¦´ì§€)
DEFAULT_ILLUSTRATION_FREQ = 3 

# 3. ê²½ë¡œ ì„¤ì • (ì‚¬ìš©ì í™˜ê²½ì— ë§ì¶° ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©)
USER_HOME = "/Users/juson"
MODEL_PATH = f"{USER_HOME}/.cache/huggingface/hub/models--mlx-community--Llama-4-Maverick-17B-16E-Instruct-6bit/snapshots/542ea389fcd614c665c4306bd60ad053d9da8d03"

# ê³µì¥ í´ë” (Factory Input)
FACTORY_DIR = f"{USER_HOME}/Desktop/factory_input"
DIR_RESULT = os.path.join(FACTORY_DIR, "1_Result")
DIR_REFERENCE = os.path.join(FACTORY_DIR, "2_Reference_Style")

# ì œë„¤ì‹œìŠ¤ í”„ë¡œì íŠ¸ í´ë”
GENESIS_PATH = f"{USER_HOME}/Desktop/Genesis_Project"
DIR_TEMPLATE = os.path.join(GENESIS_PATH, "D0_template")
DIR_FONTS = os.path.join(GENESIS_PATH, "D5_Fonts")

# â˜… Vector DB ê²½ë¡œ & ì„ë² ë”© ëª¨ë¸ (ì§€ì‹ ê²€ìƒ‰ìš©)
DB_PERSIST_DIR = f"{USER_HOME}/Desktop/Genesis_Project/99_VectorDB"
EMBEDDING_MODEL_ID = "BAAI/bge-m3"

# 4. í°íŠ¸ ì„¤ì • (ê¸°ë³¸ê°’)
# â˜… [V30 ìˆ˜ì •] Typst í˜¸í™˜ì„±ì„ ìœ„í•´ ì •í™•í•œ Family Name ì‚¬ìš© (ë„ì–´ì“°ê¸° í¬í•¨)
DEFAULT_FONT_TITLE = "Apple SD Gothic Neo" 
DEFAULT_FONT_BODY = "AppleMyungjo"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# 5. ë§ì¶¤ë²• ê²€ì‚¬ê¸° (py-hanspell)
try:
    from hanspell import spell_checker
    HANSPELL_AVAILABLE = True
except ImportError:
    HANSPELL_AVAILABLE = False
    print("âš ï¸ [System] py-hanspell ë¯¸ì„¤ì¹˜. AI êµì •ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")


# ==============================================================================
# [1] ë„êµ¬ í´ë˜ìŠ¤ (TOOLKIT)
# ==============================================================================

class StyleReplicator:
    """
    [V33 Fixed] PDF ìŠ¤íƒ€ì¼ ë¶„ì„ ë° '20ì¤„ ê³ ì •' ì •ë°€ ê³„ì‚°ê¸°
    """
    @staticmethod
    def analyze_pdf(pdf_path):
        try:
            doc = fitz.open(pdf_path)
            page = doc[0] # ì²« í˜ì´ì§€ë§Œ ë¶„ì„í•˜ì—¬ ìŠ¤íƒ€ì¼ ì¶”ì¶œ
            blocks = page.get_text("dict")["blocks"]
            font_sizes = []
            
            # í…ìŠ¤íŠ¸ ë¸”ë¡ì—ì„œ í°íŠ¸ í¬ê¸° ìˆ˜ì§‘
            for b in blocks:
                if "lines" in b:
                    for l in b["lines"]:
                        for s in l["spans"]:
                            font_sizes.append(s["size"])
            
            if not font_sizes:
                return None
            
            # í†µê³„ì  ë¶„ì„: ê°€ì¥ ë§ì´ ì“°ì¸ í¬ê¸°(ë³¸ë¬¸)ì™€ ê°€ì¥ í° í¬ê¸°(ì œëª©) ì¶”ì¶œ
            body_size = Counter(font_sizes).most_common(1)[0][0]
            title_size = max(font_sizes)
            
            # ì—¬ë°± ê³„ì‚° (ì¢Œìš° ì—¬ë°± ì¶”ì •)
            rect = page.rect
            margin_x = 72 # ê¸°ë³¸ê°’ (1ì¸ì¹˜)
            if blocks:
                left_x = blocks[0]["bbox"][0]
                right_x = blocks[0]["bbox"][2]
                margin_x = (rect.width - (right_x - left_x)) / 2
            
            # â˜… [V33 í•µì‹¬] 20ì¤„ ë§ì¶¤í˜• í–‰ê°„(Leading) ì—­ì‚° ë¡œì§ (A4 ê¸°ì¤€)
            # A4 ë†’ì´: ì•½ 842pt (297mm)
            # ìƒí•˜ ì—¬ë°±(Typography Standard): ìœ„ 3cm(85pt) + ì•„ë˜ 3cm(85pt) = 170pt
            
            vertical_margin_pt = 85.0 # ì•½ 3cm
            available_height = rect.height - (vertical_margin_pt * 2)
            target_lines = 20
            
            line_height = available_height / target_lines
            # í•„ìš”í•œ í–‰ê°„ (Leading) = í•œ ì¤„ë‹¹ ë†’ì´ - í°íŠ¸ í¬ê¸°
            calculated_leading = line_height - body_size
            
            # ê³„ì‚°ëœ ê°’ì´ ë„ˆë¬´ ì‘ê±°ë‚˜ í¬ë©´ ì•ˆì „ê°’(18pt~24pt)ìœ¼ë¡œ ë³´ì •
            if calculated_leading < 12: calculated_leading = 18.0
            if calculated_leading > 40: calculated_leading = 24.0
            
            return {
                "page_width": f"{rect.width}pt", 
                "page_height": f"{rect.height}pt",
                "margin_x": f"{margin_x}pt",
                "margin_y": f"{vertical_margin_pt}pt",
                "body_size": f"{body_size:.1f}pt", 
                "title_size": f"{title_size:.1f}pt",
                "text_size": f"{body_size}",
                "leading": f"{calculated_leading:.2f}pt" # ê³„ì‚°ëœ í–‰ê°„
            }
        except Exception as e:
            return None

class TermGuard:
    """
    ê¸ˆì¹™ì–´, ì˜¤íƒ€, AI í™˜ê°(Hallucination) íƒœê·¸ë¥¼ ì²­ì†Œí•˜ëŠ” ê²½ë¹„ì› í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    """
    @staticmethod
    def enforce(text: str) -> str:
        # ê°•ì œ êµì • ì‚¬ì „
        replacements = {
            "ë…¸ê°€": "ë…¸ì•„", 
            "ì„¸ì‹ ì": "ìƒˆì‹ ì", 
            "ê¸°ë¥¼ ì¶•ë³µí•©ë‹ˆë‹¤": "ì£¼ë‹˜ì˜ ì´ë¦„ìœ¼ë¡œ ì¶•ë³µí•©ë‹ˆë‹¤",
            "###": "", 
            "**": "",
            # â˜… [ìˆ˜ì •ë¨] eot_id ë“± AI ë‚´ë¶€ íƒœê·¸ ì™„ë²½ ì œê±°
            "|eot_id|": "", 
            "<|eot_id|>": "", 
            "eot_id": ""
        }
        
        for k, v in replacements.items():
            text = text.replace(k, v)
            
        # íŠ¹ìˆ˜ë¬¸ì ë° ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
        text = text.replace("<", "").replace(">", "")
        text = re.sub(r"\n{3,}", "\n\n", text)
        return text

    @staticmethod
    def run_spell_check(text: str) -> str:
        """Hanspell ë§ì¶¤ë²• ê²€ì‚¬ê¸° ì‹¤í–‰"""
        if not HANSPELL_AVAILABLE: 
            return text
        try:
            corrected = ""
            # ë¬¸ë‹¨ë³„ë¡œ ë‚˜ëˆ„ì–´ ê²€ì‚¬ (API ì œí•œ ê³ ë ¤)
            for line in text.split('\n'):
                if not line.strip(): 
                    corrected += "\n"
                elif len(line) < 500: 
                    corrected += spell_checker.check(line).checked + "\n"
                else: 
                    corrected += line + "\n"
            return corrected.strip()
        except: 
            return text

class TextManager:
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼ AIê°€ ì²˜ë¦¬í•˜ê¸° ì¢‹ì€ í¬ê¸°ë¡œ ìë¥´ëŠ” ê´€ë¦¬ìì…ë‹ˆë‹¤.
    """
    @staticmethod
    def split_text_clean(text, chunk_size=3000):
        chunks = []
        start = 0
        text_len = len(text)
        
        while start < text_len:
            end = min(start + chunk_size, text_len)
            
            # ë¬¸ì¥ ì¤‘ê°„ì—ì„œ ì˜ë¦¬ì§€ ì•Šë„ë¡ ë§ˆì¹¨í‘œ(.) ìœ„ì¹˜ í™•ì¸
            if end < text_len:
                last_period = text.rfind('.', start, end)
                if last_period != -1 and last_period > start + (chunk_size // 2): 
                    end = last_period + 1
            
            chunk = text[start:end].strip()
            if chunk: 
                chunks.append(chunk)
            start = end
            
        return chunks

class KnowledgeManager:
    """
    Vector DB ê²€ìƒ‰ ë° í°íŠ¸/í…œí”Œë¦¿ íŒŒì¼ì„ ê´€ë¦¬í•˜ëŠ” ì‚¬ì„œ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    """
    @staticmethod
    def scan_fonts(font_dir):
        """í°íŠ¸ í´ë”ë¥¼ ìŠ¤ìº”í•˜ì—¬ ì‚¬ìš© ê°€ëŠ¥í•œ í°íŠ¸ ëª©ë¡ì„ ë§Œë“­ë‹ˆë‹¤."""
        found = []
        if os.path.exists(font_dir):
            for root, _, files in os.walk(font_dir):
                for file in files:
                    if file.lower().endswith(('.ttf', '.otf')):
                        name = os.path.splitext(file)[0]
                        # í°íŠ¸ ì´ë¦„ ì •ê·œí™” (Bold, Regular ë“± ì œê±°)
                        name = re.sub(r"-(Bold|Regular|Light|Medium|Black|Thin|ExtraBold|SemiBold)", "", name, flags=re.IGNORECASE)
                        name = re.sub(r"([a-z])([A-Z])", r"\1 \2", name) 
                        
                        if "Noto Sans" in name: name = "Noto Sans KR"
                        if "Noto Serif" in name: name = "Noto Serif KR"
                        
                        name = name.strip()
                        if name not in found: found.append(name)
                        
        return found if found else [DEFAULT_FONT_TITLE, DEFAULT_FONT_BODY]

    @staticmethod
    def load_templates():
        """ë””ìì¸ í…œí”Œë¦¿(Typst ì½”ë“œ)ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        buf = ""
        if os.path.exists(DIR_TEMPLATE):
            for file in glob.glob(os.path.join(DIR_TEMPLATE, "*.typ")):
                try: 
                    with open(file, 'r', encoding='utf-8') as f:
                        buf += f"\n[Template Code: {os.path.basename(file)}]\n{f.read()}\n"
                except: pass
        return buf

    @staticmethod
    def search_vector_db(query: str, k: int = 5) -> str:
        """RAG: Vector DBì—ì„œ ê´€ë ¨ ì§€ì‹ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        if not os.path.exists(DB_PERSIST_DIR):
            log("Librarian", "âš ï¸ Vector DBê°€ ì—†ìŠµë‹ˆë‹¤. (ì§€ì‹ ê²€ìƒ‰ ê±´ë„ˆëœ€)")
            return ""

        log("Librarian", f"ğŸ” ì§€ì‹ ê²€ìƒ‰ ì‹¤í–‰: '{query[:30]}...'")
        try:
            embeddings = HuggingFaceEmbeddings(
                model_name=EMBEDDING_MODEL_ID,
                model_kwargs={'device': 'mps'},
                encode_kwargs={'normalize_embeddings': True}
            )
            vectordb = Chroma(
                persist_directory=DB_PERSIST_DIR, 
                embedding_function=embeddings,
                collection_name="genesis_knowledge"
            )
            docs = vectordb.similarity_search(query, k=k)
            
            context_text = ""
            for i, doc in enumerate(docs):
                # â˜… [ìˆ˜ì •ë¨] íŒŒì¼ëª… ì˜¤ì—¼ ë°©ì§€ë¥¼ ìœ„í•´ ë©”íƒ€ë°ì´í„° ì¶œì²˜ëŠ” ìˆ¨ê¹€ ì²˜ë¦¬
                context_text += f"\n[Reference {i+1}]\n{doc.page_content}\n"
            
            log("Librarian", f"âœ… ê´€ë ¨ ë¬¸ì„œ {len(docs)}ê±´ í™•ë³´ ì™„ë£Œ.")
            return context_text
        except Exception as e:
            log("Librarian", f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""

    @staticmethod
    def fix_typst_syntax(code: str) -> str:
        """
        [V16] Typst ì½”ë“œì˜ ë¬¸ë²• ì˜¤ë¥˜ ë° ë¶ˆí•„ìš”í•œ ì°Œêº¼ê¸°ë¥¼ ì™„ë²½í•˜ê²Œ ì œê±°í•©ë‹ˆë‹¤.
        """
        cleaned_lines = []
        for line in code.split('\n'):
            stripped = line.strip()
            # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
            if stripped.startswith("###") or stripped.startswith("##") or stripped.startswith("```"):
                continue
            cleaned_lines.append(line)
        
        code = "\n".join(cleaned_lines)

        # 1. êµ¬ë²„ì „ ë¬¸ë²•(locate)ì„ ì‹ ë²„ì „(#context)ìœ¼ë¡œ êµì²´
        code = code.replace("locate(loc =>", "#context")
        code = re.sub(r"#?locate\s*\(\s*\w+\s*=>", "#context", code)
        
        # 2. ê´„í˜¸ ì¤‘ì²© ì—ëŸ¬ í•´ê²° (loc ë³€ìˆ˜ ê°•ì œ ì‚­ì œ)
        code = code.replace(", loc)", ")") 
        code = code.replace(",loc)", ")")
        
        # 3. ë‚˜ë¨¸ì§€ ë¬¸ë²• ì •ë¦¬
        code = code.replace(".at(loc)", ".get()")
        code = code.replace("locate(heading)", "heading")
        code = code.replace("indent: true,", "")
        code = code.replace("indent: true", "")
        
        return code
    
def log(agent, msg):
    """ë¡œê·¸ ì¶œë ¥ í•¨ìˆ˜"""
    icons = {"Director": "ğŸ¬", "Librarian": "ğŸ“š", "Editor": "âœï¸", "Designer": "ğŸ¨", "System": "âš™ï¸", "Replicator": "ğŸ§¬", "Illustrator": "ğŸ–Œï¸"}
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {icons.get(agent, 'ğŸ¤–')} [{agent}] {msg}")

def cleanup_on_start():
    """ì‹œì‘ ì‹œ í´ë” ì •ë¦¬ ë° ì”ì—¬ íŒŒì¼ ì²˜ë¦¬"""
    for d in [FACTORY_DIR, DIR_RESULT, DIR_REFERENCE, DIR_TEMPLATE]:
        if not os.path.exists(d): os.makedirs(d)
        
    target = os.path.join(FACTORY_DIR, "pc_output.json")
    if os.path.exists(target):
        try:
            timestamp = int(time.time())
            os.rename(target, os.path.join(FACTORY_DIR, f"ignored_{timestamp}.json"))
        except: pass

# ==============================================================================
# [2] ìƒíƒœ ê´€ë¦¬ (State Management)
# ==============================================================================
class BookState(TypedDict):
    user_instruction: str; 
    raw_material: str;
    knowledge_context: str; 
    available_fonts: List[str]
    book_title: str; 
    text_chunks: List[str]
    polished_chunks: List[Tuple[str, str]]
    current_chunk_idx: int;
    replicated_template_name: str; 
    illustration_freq: int

# ==============================================================================
# [3] AI ì—ì´ì „íŠ¸ (AGENTS)
# ==============================================================================

def load_model_once():
    """ëª¨ë¸ ë¡œë”© (ìµœì´ˆ 1íšŒ)"""
    try:
        log("System", "Maverick(Llama-4) ì—”ì§„ ì˜ˆì—´ ì¤‘...")
        return load(MODEL_PATH)
    except: return None, None

def agent_librarian(model, tokenizer, state: BookState) -> BookState:
    """
    [ì‚¬ì„œ] ì§€ì‹ ê²€ìƒ‰ ë° í°íŠ¸/í…œí”Œë¦¿ ì¤€ë¹„
    """
    log("Librarian", "ì§€ì‹ ë„ì„œê´€ ì ‘ì† ì¤‘...")
    state['available_fonts'] = KnowledgeManager.scan_fonts(DIR_FONTS)
    tpl_context = KnowledgeManager.load_templates()
    
    # RAG ê²€ìƒ‰ ìˆ˜í–‰
    search_query = f"{state['user_instruction']} {state['raw_material'][:500]}"
    retrieved_knowledge = KnowledgeManager.search_vector_db(search_query, k=6)
    
    state['knowledge_context'] = tpl_context + "\n" + retrieved_knowledge
    return state

def agent_replicator(model, tokenizer, state: BookState) -> BookState:
    """
    [ë³µì œì] V36: ì§€ì‹ì €ì¥ì†Œ(D5_Fonts) í°íŠ¸ ìµœìš°ì„  ì ìš© & 20ì¤„ ê·¸ë¦¬ë“œ
    """
    state['replicated_template_name'] = ""
    
    # PDF ë¶„ì„
    pdf_files = glob.glob(os.path.join(DIR_REFERENCE, "*.pdf"))
    if not pdf_files:
        log("Replicator", "âš ï¸ ìŠ¤íƒ€ì¼ ì°¸ê³ ìš© PDFê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©.")
        analysis = {"margin_x": "3cm", "margin_y": "3cm", "text_size": "10.5", "leading": "22pt"}
    else:
        target_pdf = random.choice(pdf_files)
        log("Replicator", f"ìŠ¤íƒ€ì¼ ì¶”ì¶œ ëŒ€ìƒ: {os.path.basename(target_pdf)}")
        analysis = StyleReplicator.analyze_pdf(target_pdf)
        if not analysis:
             analysis = {"margin_x": "3cm", "margin_y": "3cm", "text_size": "10.5", "leading": "22pt"}
    
    # â˜… [V36 í•µì‹¬ ìˆ˜ì •] í°íŠ¸ ìš°ì„ ìˆœìœ„ ë³€ê²½
    # ê¸°ì¡´: (ê¸°ë³¸í°íŠ¸, ì‚¬ìš©ìí°íŠ¸) -> ê¸°ë³¸í°íŠ¸ê°€ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ë‹¹ì²¨ë¨
    # ë³€ê²½: (ì‚¬ìš©ìí°íŠ¸, ê¸°ë³¸í°íŠ¸) -> ì‚¬ìš©ìí°íŠ¸ë¥¼ ë¨¼ì € ì‹œë„í•¨
    
    available = state['available_fonts']
    
    # í°íŠ¸ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´
    if available:
        # ì‚¬ìš©ì í°íŠ¸ë¥¼ ë§¨ ì•ì— ë°°ì¹˜ (ë”°ì˜´í‘œ ì²˜ë¦¬)
        formatted_fonts = [f'"{f}"' for f in available[:3]]
        fonts_str = ", ".join(formatted_fonts)
        # ì‚¬ìš©ì í°íŠ¸ ë’¤ì— ì•ˆì „ì¥ì¹˜ë¡œ ê¸°ë³¸ í°íŠ¸ ì¶”ê°€
        font_array_code = f'({fonts_str}, "{DEFAULT_FONT_BODY}")'
        log("Replicator", f"ğŸ”¤ í°íŠ¸ ìš°ì„ ìˆœìœ„ ì ìš©: {available[0]} (ì‚¬ìš©ì ì§€ì •)")
    else:
        font_array_code = f'("{DEFAULT_FONT_BODY}")'
        log("Replicator", "âš ï¸ ì‚¬ìš©ì í°íŠ¸ ì—†ìŒ. ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©.")
    
    typ_code = f"""
    // [Genesis Layout Engine V36: Font Priority Fix]
    #let grid-leading = {analysis['leading']}
    #let standard-leading = 0.8em 
    
    #set page(
      paper: "a4",
      margin: (x: {analysis['margin_x']}, y: {analysis['margin_y']}),
      numbering: "1"
    )
    
    #set text(
      font: {font_array_code}, // â˜… ì‚¬ìš©ì í°íŠ¸ê°€ 1ìˆœìœ„ë¡œ ë“¤ì–´ê°
      size: {analysis['text_size']}pt, 
      lang: "ko"
    )
    
    #set par(
      justify: true, 
      first-line-indent: 1em, 
      leading: grid-leading 
    )
    
    #let chapter_title(content) = {{
      pagebreak(weak: true)
      v(4em)
      // ì œëª©ìš© í°íŠ¸ë„ ì‚¬ìš©ì í°íŠ¸ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ìš°ì„  ì‚¬ìš©
      text(size: 20pt, weight: "bold")[#content] 
      v(2em)
    }}
    """
    
    template_name = f"replicated_{int(time.time())}.typ"
    with open(os.path.join(DIR_TEMPLATE, template_name), "w", encoding="utf-8") as f: 
        f.write(typ_code)
    state['replicated_template_name'] = template_name
    log("Replicator", "âœ… ìŠ¤íƒ€ì¼ í…œí”Œë¦¿ ìƒì„± ì™„ë£Œ.")
    return state

def agent_director(model, tokenizer, state: BookState) -> BookState:
    """
    [ê¸°íšì] â˜… V35: ë§¥ë½ ì´í•´ ê¸°ë°˜ ì œëª© ì¶”ì¶œ (LLM Context Awareness)
    """
    if not state['raw_material']: return state
    log("Director", "ê¸°íš ë° ì „ì²˜ë¦¬ ì¤‘... (ì œëª© ë§¥ë½ ë¶„ì„ ì¤‘)")
    
    instruction = state['user_instruction']
    forced_title = ""

    # 1. 1ì°¨ ì‹œë„: ì •ê·œì‹ (ë¹ ë¥¸ ê²€ì‚¬)
    patterns = [
        r"^\s*(?:ì œëª©|Title|ì±…\s*ì œëª©)\s*[:=ï¼š]\s*(.*?)$",
        r"^\s*ì œëª©ì€\s*(.*?)\s*(?:ì•¼|ì´ì•¼|ë¡œ\s*í•´ì¤˜|ì…ë‹ˆë‹¤|ì´ë‹¤)$"
    ]
    for pat in patterns:
        match = re.search(pat, instruction, re.MULTILINE | re.IGNORECASE)
        if match:
            forced_title = match.group(1).strip().strip('"\'.,')
            forced_title = re.sub(r"(ë¼ê³ |ìœ¼ë¡œ)?\s*(í•´ì¤˜|ì§€ì–´ì¤˜|ë¶€íƒí•´)$", "", forced_title).strip()
            break
    
    # 2. â˜… [V35 í•µì‹¬] LLMì—ê²Œ ì§ì ‘ ë¬¼ì–´ë³´ê¸° (Context Awareness)
    # ì •ê·œì‹ì´ ì‹¤íŒ¨í–ˆë‹¤ë©´, AIì—ê²Œ ì§€ì‹œì‚¬í•­ì„ ì½ê³  ì œëª©ì„ ì°¾ì•„ë‚´ë¼ê³  ì‹œí‚´
    if not forced_title:
        log("Director", "ğŸ•µï¸â€â™‚ï¸ ì œëª© ë§¥ë½ ì •ë°€ ë¶„ì„ ì¤‘ (AI Reading)...")
        prompt = f"""<|system|>You are a helpful assistant. Read the user's instruction and identify if a specific Book Title is requested.
[User Instruction]:
{instruction}

[Task]:
- If the user explicitly stated a title (e.g., "Title is X", "Make the title X", "ì œëª©: X"), output ONLY that title.
- If the user did NOT specify a title, output exactly "NONE".

[Examples]:
- Input: "ì œëª©ì€ ê¹€ëª¨ì„¸ë¡œ í•´ì¤˜" -> Output: "ê¹€ëª¨ì„¸"
- Input: "êµì • ì˜ ë¶€íƒí•´" -> Output: "NONE"
<|user|>Extract Title<|assistant|>"""
        
        try:
            # 50í† í° ì •ë„ë§Œ ìƒì„±í•´ì„œ ì§§ê²Œ ë‹µë³€ ë°›ìŒ
            extracted = generate(model, tokenizer, prompt=prompt, max_tokens=50, verbose=False).strip()
            
            # "NONE"ì´ ì•„ë‹ˆê³ , ìœ ì˜ë¯¸í•œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì œëª©ìœ¼ë¡œ ì±„íƒ
            if "NONE" not in extracted and len(extracted) > 1:
                # ë¶ˆí•„ìš”í•œ ë”°ì˜´í‘œ ì œê±°
                forced_title = extracted.strip('"\' ')
                log("Director", f"ğŸ§© ë§¥ë½ íŒŒì•… ì™„ë£Œ: '{forced_title}'")
        except: pass

    # 3. ê²°ê³¼ ì ìš© (ì‚¬ìš©ì ì§€ì • or AI ì°½ì‘)
    if forced_title:
        state['book_title'] = forced_title
        log("Director", f"ğŸ”’ ì‚¬ìš©ì ì œëª© ê°•ì œ ì ìš©: {state['book_title']}")
    else:
        # ì§„ì§œ ì—†ì„ ë•Œë§Œ ì°½ì‘
        prompt = f"""<|system|>You are a Chief Editor. Determine the book title.
[User Instruction]: "{state['user_instruction']}"
[Content Preview]: {state['raw_material'][:2000]}
[STRICT RULES]: Create a best-selling title. Output ONLY the final title text.
<|user|>Determine Title<|assistant|>"""
        
        title_raw = generate(model, tokenizer, prompt=prompt, max_tokens=100, verbose=False).strip()
        title_clean = title_raw.split("<|")[0]
        title_clean = re.sub(r"^(Title|Subject|ì œëª©|ì±…\s*ì œëª©|ê²°ê³¼)\s*[:ï¼š]\s*", "", title_clean, flags=re.IGNORECASE)
        state['book_title'] = title_clean.strip('"\' ')
        log("Director", f"ğŸ§  AI ê¸°íš ì œëª©: {state['book_title']}")
    
    state['text_chunks'] = TextManager.split_text_clean(state['raw_material'])
    return state

def agent_hybrid_editor(model, tokenizer, state: BookState) -> BookState:
    """
    [í¸ì§‘ì] í…ìŠ¤íŠ¸ ìœ¤ë¬¸ ë° êµì •
    """
    idx = state['current_chunk_idx']
    if idx >= len(state['text_chunks']): return state
    
    current_text = state['text_chunks'][idx]
    log("Editor", f"ìœ¤ë¬¸ ì‘ì—… ì¤‘... [{idx+1}/{len(state['text_chunks'])}]")
    
    prev_text = ""
    if state['polished_chunks']:
        for t, c in reversed(state['polished_chunks']):
            if t == 'text':
                prev_text = c[-300:]
                break
    
    prompt = f"""<|system|>ìˆ˜ì„ í¸ì§‘ì¥ì…ë‹ˆë‹¤. ë¬¸ë§¥ ë³µì› ë° ìœ¤ë¬¸.
[ì°¸ê³  ì§€ì‹]: {state['knowledge_context'][:2000]}
[ì›ì¹™] ë¬¸ë§¥ ë³µì›, ë¬¸ì–´ì²´, ë¬¸ë‹¨ êµ¬ë¶„.
<|user|>[ì´ì „]:...{prev_text}\n[ì›ë¬¸]:{current_text}\n[ì§€ì‹œ]:ìœ¤ë¬¸í•˜ë¼.<|assistant|>"""
    
    draft = generate(model, tokenizer, prompt=prompt, max_tokens=4000, verbose=False)
    
    # â˜… íƒœê·¸ ì²­ì†Œ ë° ì‚¬ì¡± ì œê±°
    draft = draft.split("<|")[0]
    draft = re.sub(r"^(ë„¤|ë¬¼ë¡ |ì•Œê² |í™•ì¸|ìˆ˜ì •|ìœ¤ë¬¸|ì œì‹œ|ë”°ë¼ì„œ).+?(\n|$)", "", draft, flags=re.MULTILINE).strip()
    
    # ë§ì¶¤ë²• ê²€ì‚¬
    if HANSPELL_AVAILABLE: 
        draft = TermGuard.run_spell_check(draft)
    
    # ìµœì¢… ê¸ˆì¹™ì–´ êµì •
    final_text = TermGuard.enforce(draft)
    
    state['polished_chunks'].append(('text', final_text))
    state['current_chunk_idx'] += 1
    return state

def agent_illustrator(model, tokenizer, state: BookState) -> BookState:
    """
    â˜… [V33 Upgrade] í† í° ì œí•œ(77 Limit) í•´ê²° + ì‹œë„¤ë§ˆí‹± ì‚¬ì§„ + ì•ˆì „ì¥ì¹˜
    """
    processed_count = len(state['polished_chunks'])
    if not state['polished_chunks']: return state

    last_type, last_content = state['polished_chunks'][-1]
    freq = state.get('illustration_freq', DEFAULT_ILLUSTRATION_FREQ)

    if last_type == 'text' and (processed_count % freq == 0):
        log("Illustrator", f"ğŸ“· ì„¤êµ ë¬µìƒ ë° í‚¤ì›Œë“œ ìµœì í™” í”„ë¡¬í”„íŠ¸ ì‘ì„± (ë¹ˆë„: {freq})...")
        
        # í‚¤ì›Œë“œ ì¤‘ì‹¬(Keyword-only) í”„ë¡¬í”„íŠ¸ ìƒì„± (77í† í° ì œí•œ í•´ê²°)
        prompt_desc = f"""<|system|>You are a Master Photographer specializing in Cinematic Biblical Realism.
Your task is to create a prompt for a hyper-realistic photograph.

[Input Sermon Text]:
{last_content[:700]}

[CRITICAL RULES]:
1. **NO SENTENCES.** Do not use "A photo of...", "The scene shows...", "In the background...".
2. **USE KEYWORDS ONLY.** Separate with commas. Focus on visual objects.
3. **KEEP IT SHORT.** Maximum 50 words to avoid truncation errors.

[Examples]:
- Bad: "A beautiful image showing a seed growing in the ground representing faith." (Too long)
- Good: "close-up shot, hand planting gold coin, fertile soil, sunrise lighting, cinematic, 8k, sharp focus"

<|user|>Create Keyword Prompt<|assistant|>"""
        
        try:
            # max_tokensë¥¼ ì¤„ì—¬ì„œ AIê°€ ê¸¸ê²Œ ë§í•˜ëŠ” ê²ƒì„ ì›ì²œ ì°¨ë‹¨
            visual_prompt = generate(model, tokenizer, prompt=prompt_desc, max_tokens=100, verbose=False).strip()
        except: visual_prompt = "Error"

        # í•œê¸€/ì˜¤ë¥˜ ë°©ì–´ ë¡œì§ (í‚¤ì›Œë“œ í˜•íƒœë¡œ ë³€ê²½ë¨)
        if re.search(r'[ê°€-í£]', visual_prompt) or len(visual_prompt) < 10 or "Error" in visual_prompt:
            visual_prompt = "sacred biblical scene, divine light, grace, hyper-realistic, 8k resolution, dramatic lighting, sharp focus, solemn atmosphere"
        
        # ì¡ë‹¤í•œ ì„¤ëª… ì œê±°
        visual_prompt = re.sub(r'^(Here is|Prompt:|Visual Metaphor:|Keywords:).*?[\:\n]', '', visual_prompt).strip()
        
        # â˜… [í•µì‹¬ ìˆ˜ì •] ìŠ¤íƒ€ì¼ íƒœê·¸ ì „ì§„ ë°°ì¹˜ (Front-Loading)
        final_prompt = f"photorealistic, 8k uhd, cinematic lighting, {visual_prompt}, highly detailed, sharp focus, masterpiece, sacred art"
        
        log("Illustrator", f"ì˜ë¢°ì„œ(í† í° ìµœì í™”): {final_prompt[:60]}...")

        try:
            # â˜… [í•„ìˆ˜ ì•ˆì „ì¥ì¹˜ 2] íƒ€ì„ì•„ì›ƒ 180ì´ˆ (PCê°€ VRAM ì •ë¦¬í•˜ëŠë¼ ëŠë¦´ ë•Œë¥¼ ëŒ€ë¹„)
            res = requests.post(PC_FLUX_SERVER_URL, json={"prompt": final_prompt}, timeout=180)
            if res.status_code == 200:
                fname = res.json().get("filename")
                if fname:
                    log("Illustrator", f"âœ… ì‚¬ì§„ ë„ì°©: {fname}")
                    state['polished_chunks'].append(('image', fname))
            else: 
                log("Illustrator", f"âš ï¸ ì„œë²„ ì˜¤ë¥˜: {res.status_code}")
        except requests.exceptions.Timeout:
            log("Illustrator", "â° ì‹œê°„ ì´ˆê³¼ (PCê°€ ë°”ì¨, ì‚¬ì§„ ìƒëµ)")
        except Exception as e: 
            log("Illustrator", f"âŒ ì—°ê²° ì‹¤íŒ¨: {e}")
            
    return state

def agent_designer(model, tokenizer, state: BookState) -> bool:
    """
    [ë””ìì´ë„ˆ] V33: ë‹¤ì´ë‚´ë¯¹ ë ˆì´ì•„ì›ƒ (ì´ë¯¸ì§€ ìŠ¤ì½”í”„ ì ìš©) & í°íŠ¸ëª… ì•ˆì „ ì²˜ë¦¬
    """
    log("Designer", "ğŸ“š ìµœì¢… ì¡°íŒ ì‘ì—… ì‹œì‘ (ë‹¤ì´ë‚´ë¯¹ ë ˆì´ì•„ì›ƒ ì ìš©).")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_file = os.path.join(DIR_RESULT, f"GenesisBook_{timestamp}")
    
    state['available_fonts'] = KnowledgeManager.scan_fonts(DIR_FONTS)
    
    style_code = ""
    if state.get('replicated_template_name'):
        try:
            tpl_path = os.path.join(DIR_TEMPLATE, state['replicated_template_name'])
            with open(tpl_path, 'r', encoding='utf-8') as f:
                style_code = f.read()
        except: pass

    content_code = ""
    for c_type, content in state['polished_chunks']:
        if c_type == 'text':
            clean = content.replace("#", "").replace("*", "").replace("`", "") 
            clean = clean.replace("[", "\\[").replace("]", "\\]")
            content_code += f"{clean}\n\n"
        elif c_type == 'image':
            if os.path.exists(os.path.join(FACTORY_DIR, content)):
                # â˜… ì´ë¯¸ì§€ ì‚½ì… êµ¬ê°„ ìŠ¤ì½”í”„ ì²˜ë¦¬ (V27/V33)
                content_code += f"""
                #[
                  #set par(leading: standard-leading)
                  #v(1em)
                  #figure(
                    image("../{content}", width: 85%),
                    caption: [ì˜ì  ì‹œê°í™”]
                  )
                  #v(1em)
                ]
                \n
                """

    full_typst = f"""
    {style_code}
    
    // ë„ë¹„ë¼(ì†í‘œì§€)
    #align(center + horizon)[
      // â˜… í°íŠ¸ëª… ì—ëŸ¬ í•´ê²° (ë³€ìˆ˜ ì‚¬ìš©)
      #text(size: 24pt, weight: "bold", font: "{DEFAULT_FONT_TITLE}")[{state['book_title']}]
      #v(2em)
      #text(size: 12pt)[GENESIS AI WRITER]
    ]
    
    // ì±•í„° ì œëª© ì ìš©
    #chapter_title[{state['book_title']}]
    
    {content_code}
    """
    
    with open(f"{out_file}.typ", "w", encoding="utf-8") as f: f.write(full_typst)
    try:
        subprocess.run(["typst", "compile", f"{out_file}.typ", f"{out_file}.pdf", "--root", FACTORY_DIR, "--font-path", DIR_FONTS], check=True)
        log("Designer", f"ğŸ‰ ì±… ì™„ì„±: {os.path.basename(out_file)}.pdf")
        return True
    except Exception as e:
        log("Designer", f"âŒ ì»´íŒŒì¼ ì‹¤íŒ¨: {e}")
        return False

# ==============================================================================
# [5] ë©”ì¸ ë£¨í”„ (V18: PHANTOM SIGNAL FIX & TEXT CHECK)
# ==============================================================================
def run_genesis_architect(model, tokenizer, json_path):
    """
    ë‹¨ì¼ ì‘ì—…ì„ ì²˜ë¦¬í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜
    """
    with open(json_path, 'r', encoding='utf-8') as f:
        input_json = json.load(f)

    custom_freq = DEFAULT_ILLUSTRATION_FREQ
    if "frequency" in input_json:
        try: custom_freq = int(input_json["frequency"])
        except: pass

    # ì—ëŸ¬ ë³´ê³  íŒŒì¼ì€ ë¬´ì‹œ
    if input_json.get("status") == "error":
        log("System", f"ğŸ›‘ ì˜¤ë¥˜ ë³´ê³  ë¬´ì‹œ: {input_json.get('message')}")
        return False

    # 1. í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”© ë¡œì§
    raw_text = input_json.get("script_ko", "") or input_json.get("script", "")
    text_filename = input_json.get("text_file", "")
    
    # í…ìŠ¤íŠ¸ íŒŒì¼ì´ ëª…ì‹œëœ ê²½ìš° ì½ê¸° ì‹œë„
    if not raw_text and text_filename:
        txt_path = os.path.join(FACTORY_DIR, text_filename)
        # íŒŒì¼ì´ ì „ì†¡ë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸° (ìµœëŒ€ 5ì´ˆ)
        for _ in range(5): 
            if os.path.exists(txt_path): break
            time.sleep(1)
            
        if os.path.exists(txt_path):
            log("System", f"ğŸ“‚ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ì„±ê³µ: {text_filename}")
            with open(txt_path, 'r', encoding='utf-8') as f:
                raw_text = f.read()
        else:
            log("System", f"âš ï¸ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {text_filename}")
            return False 

    # â˜… 2. í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ì¦‰ì‹œ ì¢…ë£Œ (False ë°˜í™˜) -> ë©”ì¸ ë£¨í”„ì—ì„œ ë¬´ì‹œ ì²˜ë¦¬ë¨
    if not raw_text:
        return False

    # 3. ìƒíƒœ ì´ˆê¸°í™”
    state: BookState = {
        "user_instruction": input_json.get("script", ""), 
        "raw_material": raw_text,  
        "img_snap": input_json.get("image_source", ""),
        "img_flux": input_json.get("flux_source", ""),
        "learned_style": "", "knowledge_context": "", "available_fonts": [],
        "book_title": "", "text_chunks": [], "polished_chunks": [],
        "current_chunk_idx": 0, "layout_config": {}, 
        "replicated_template_name": "", "selected_style_name": "",
        "illustration_freq": custom_freq
    }
    
    # ì—ì´ì „íŠ¸ ìˆœì°¨ ì‹¤í–‰
    state = agent_librarian(model, tokenizer, state) 
    state = agent_replicator(model, tokenizer, state)
    state = agent_director(model, tokenizer, state)
    
    total = len(state['text_chunks'])
    while state['current_chunk_idx'] < total:
        state = agent_hybrid_editor(model, tokenizer, state)
        state = agent_illustrator(model, tokenizer, state)
    
    return agent_designer(model, tokenizer, state)

def main():
    """
    í”„ë¡œê·¸ë¨ ì§„ì…ì : íŒŒì¼ ê°ì‹œ ë° ì‘ì—… ìŠ¤ì¼€ì¤„ë§
    """
    print("\n" + "="*80)
    print(" ğŸ›ï¸  [GENESIS WRITER V34: ALL FEATURES RESTORED]")
    print(f"     Monitoring: {FACTORY_DIR}")
    print("     Status: ğŸŸ¢ Waiting for TEXT content...")
    print("="*80)
    
    model, tokenizer = load_model_once()
    if not model: return
    cleanup_on_start()
    
    while True:
        try:
            files = os.listdir(FACTORY_DIR)
            
            # ì˜¤ì§ pc_output.jsonë§Œ ì²˜ë¦¬
            if "pc_output.json" in files:
                target_path = os.path.join(FACTORY_DIR, "pc_output.json")
                processing_path = os.path.join(FACTORY_DIR, "processing.json")
                
                try:
                    # 1. íŒŒì¼ ì„ ì  (ì´ë¦„ ë³€ê²½)
                    os.rename(target_path, processing_path)
                    
                    # 2. ë‚´ìš© í™•ì¸ (í…ìŠ¤íŠ¸ ì—†ìœ¼ë©´ ë¬´ì‹œ)
                    with open(processing_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # í…ìŠ¤íŠ¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í•„í„°
                    has_text = data.get("script_ko") or data.get("script") or data.get("text_file")
                    
                    if not has_text:
                        # í…ìŠ¤íŠ¸ ì—†ëŠ” ì‹ í˜¸ëŠ” ì¡°ìš©íˆ ë¬´ì‹œ (ë¡œê·¸ ì•ˆ ë‚¨ê¹€)
                        ignored_name = f"ignored_signal_{int(time.time())}.json"
                        os.rename(processing_path, os.path.join(FACTORY_DIR, ignored_name))
                        continue

                    # 3. ì§„ì§œ ì‘ì—… ì‹œì‘
                    log("System", "ğŸš€ ì›ê³  ë°œê²¬! ì§‘í•„ ì‹œì‘...")
                    success = run_genesis_architect(model, tokenizer, processing_path)
                    
                    if success:
                        final_name = f"done_{int(time.time())}.json"
                        os.rename(processing_path, os.path.join(FACTORY_DIR, final_name))
                        log("System", "âœ… ì‘ì—… ì™„ë£Œ. ëŒ€ê¸° ëª¨ë“œ ì „í™˜.")
                    else:
                        error_name = f"error_{int(time.time())}.json"
                        os.rename(processing_path, os.path.join(FACTORY_DIR, error_name))
                        log("System", "âŒ ì‘ì—… ì‹¤íŒ¨ (ì—ëŸ¬ íŒŒì¼ ë³´ê´€).")
                        
                except OSError: pass
                except Exception as e:
                    log("System", f"âš ï¸ ì˜¤ë¥˜: {e}")

        except Exception: pass
        time.sleep(1)

if __name__ == "__main__":
    main()
